<?xml version="1.0" encoding="UTF-8"?>
<cursorrules>
<purpose>
  You are an AI assistant responsible for helping a developer maintain Python code quality and develop an async-first CLI tool using modern Python practices, with a focus on AI/ML integration through LangChain and LangGraph.
</purpose>

<instructions>
  <instruction>Follow the established project structure with src/contextforge_cli as the main package.</instruction>
  <instruction>Maintain separation of concerns between different modules and components.</instruction>
  <instruction>Use UV (https://docs.astral.sh/uv) for dependency management and virtual environments.</instruction>
  <instruction>Follow the established directory structure for tests, documentation, and source code.</instruction>
  <instruction>Ensure all new code follows the project's typing and documentation standards.</instruction>
  <instruction>Implement async-first design patterns throughout the codebase.</instruction>
  <instruction>Utilize LangChain and LangGraph for AI/ML integrations.</instruction>
</instructions>

<python_standards>
  <project_structure>
    <standard>Maintain clear project structure with separate directories for source code (`src/contextforge_cli/`), tests (`tests/`), and documentation.</standard>
    <standard>Keep configuration files in the root directory (pyproject.toml, tox.ini, mypy.ini, etc.).</standard>
    <standard>Place utility scripts in the `scripts/` directory.</standard>
    <standard>Store IDE and tool configurations in `hack/` directory.</standard>
    <standard>Use modular design with distinct files for models, services, controllers, and utilities.</standard>
    <standard>Follow composition over inheritance principle.</standard>
    <standard>Use design patterns like Adapter, Decorator, and Bridge when appropriate.</standard>
    <standard>Keep vendored code in `src/contextforge_cli/vendored/` directory.</standard>
    <standard>Organize source code into logical modules:
      - bot_logger/: Logging components
      - models/: Data models and schemas
      - shell/: Shell interaction components
      - subcommands/: CLI subcommand implementations
      - utils/: Utility functions and helpers
    </standard>
  </project_structure>

  <async_standards>
    <standard>Use async/await patterns consistently throughout the codebase.</standard>
    <standard>Implement proper async context managers for resource management.</standard>
    <standard>Use aiofiles for all file operations in async contexts.</standard>
    <standard>Implement proper error handling for async operations.</standard>
    <standard>Use asyncio.gather for concurrent operations when appropriate.</standard>
    <standard>Implement proper cancellation handling in async operations.</standard>
    <standard>Use AsyncTyperImproved for CLI command implementations.</standard>
    <example>
      <![CDATA[
      async def process_file(path: Path) -> str:
          """Process a file asynchronously.

          Args:
              path: Path to the file

          Returns:
              str: Processed content

          Raises:
              FileNotFoundError: If file doesn't exist
          """
          async with aiofiles.open(path, mode='r') as f:
              content = await f.read()
          return content
      ]]>
    </example>
  </async_standards>

  <ai_integration_standards>
    <standard>Use LangChain for structured AI/ML workflows.</standard>
    <standard>Implement LangGraph for complex AI agent systems.</standard>
    <standard>Use proper error handling for AI model calls.</standard>
    <standard>Implement retry logic for API failures.</standard>
    <standard>Use streaming responses when appropriate.</standard>
    <standard>Properly handle API rate limits and quotas.</standard>

    <langchain_examples>
      <example name="chain_with_memory">
        <![CDATA[
        from typing import Dict, List, Optional, Any
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_openai import ChatOpenAI
        from langchain_core.runnables import RunnableSequence
        from langchain.memory import ConversationBufferMemory

        async def create_chat_chain(
            system_prompt: str,
            memory: Optional[ConversationBufferMemory] = None
        ) -> RunnableSequence:
            """Create a chat chain with memory using LCEL.

            Args:
                system_prompt: The system prompt to guide the model's behavior
                memory: Optional conversation memory to maintain context

            Returns:
                RunnableSequence: A configured chat chain
            """
            # Initialize the chat model
            model = ChatOpenAI(
                model_name="gpt-4-turbo-preview",
                temperature=0.1
            )

            # Create the prompt template
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{question}")
            ])

            # Create the chain using LCEL
            chain = prompt | model | StrOutputParser()

            if memory:
                chain = chain.with_memory(memory)

            return chain

        async def invoke_chain(
            chain: RunnableSequence,
            question: str,
            **kwargs: Any
        ) -> str:
            """Invoke the chat chain with proper error handling.

            Args:
                chain: The configured chat chain
                question: User's question
                **kwargs: Additional arguments for chain invocation

            Returns:
                str: Model's response

            Raises:
                Exception: If chain invocation fails
            """
            try:
                response = await chain.ainvoke(
                    {"question": question, **kwargs}
                )
                return response
            except Exception as e:
                logger.exception("Chain invocation failed", error=str(e))
                raise
      ]]>
      </example>

      <example name="streaming_response_handler">
        <![CDATA[
        from typing import AsyncGenerator, Dict
        from langchain.callbacks.base import BaseCallbackHandler
        from langchain.schema.messages import BaseMessage

        class StreamingHandler(BaseCallbackHandler):
            """Custom handler for streaming LLM responses.

            This handler processes token-by-token streaming responses from the LLM
            and yields them through an async generator.
            """

            def __init__(self) -> None:
                self.tokens: List[str] = []

            async def on_llm_new_token(self, token: str, **kwargs: Dict) -> None:
                """Process each new token as it arrives.

                Args:
                    token: The new token from the LLM
                    kwargs: Additional callback arguments
                """
                self.tokens.append(token)

            async def get_tokens(self) -> AsyncGenerator[str, None]:
                """Yield collected tokens as they arrive.

                Yields:
                    str: Individual tokens from the LLM response
                """
                for token in self.tokens:
                    yield token
        ]]>
      </example>

      <example name="retry_wrapper">
        <![CDATA[
        from typing import Any, Callable, TypeVar
        import asyncio
        from tenacity import (
            retry,
            stop_after_attempt,
            wait_exponential,
            retry_if_exception_type
        )

        T = TypeVar("T")

        def create_retry_wrapper(
            max_attempts: int = 3,
            base_wait: float = 1.0
        ) -> Callable[[Callable[..., T]], Callable[..., T]]:
            """Create a retry decorator for LLM API calls.

            Args:
                max_attempts: Maximum number of retry attempts
                base_wait: Base wait time for exponential backoff

            Returns:
                Callable: A retry decorator configured with the specified parameters
            """
            return retry(
                stop=stop_after_attempt(max_attempts),
                wait=wait_exponential(multiplier=base_wait),
                retry=retry_if_exception_type(
                    (
                        TimeoutError,
                        ConnectionError,
                        asyncio.TimeoutError
                    )
                )
            )
        ]]>
      </example>
    </langchain_examples>

    <langgraph_examples>
      <example name="agent_workflow">
        <![CDATA[
        from typing import Dict, List, Tuple, TypedDict, Annotated, Sequence
        from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
        from langchain_openai import ChatOpenAI
        from langgraph.graph import StateGraph, END
        from langchain.tools import BaseTool
        from langchain_core.tools import tool

        class AgentState(TypedDict):
            """Type definition for agent state."""
            messages: List[BaseMessage]
            next_step: str
            tools: List[BaseTool]
            memory: Dict[str, Any]

        @tool
        def search_tool(query: str) -> str:
            """Search for information.

            Args:
                query: Search query

            Returns:
                str: Search results
            """
            # Implement search logic
            return f"Results for: {query}"

        async def process_message(
            state: AgentState,
            llm: ChatOpenAI
        ) -> AgentState:
            """Process the latest message and decide next action.

            Args:
                state: Current agent state
                llm: Language model

            Returns:
                Updated agent state
            """
            # Get the last message
            last_message = state["messages"][-1]

            # Generate response using the LLM
            response = await llm.ainvoke(
                [
                    SystemMessage(content="You are a helpful assistant."),
                    *state["messages"]
                ]
            )

            # Update state
            state["messages"].append(response)
            state["next_step"] = "evaluate"

            return state

        async def create_agent_workflow(
            tools: List[BaseTool]
        ) -> StateGraph:
            """Create a workflow graph for an AI agent system.

            Args:
                tools: List of tools available to the agent

            Returns:
                StateGraph: A configured workflow graph
            """
            # Initialize workflow
            workflow = StateGraph(AgentState)

            # Add nodes
            workflow.add_node("process", process_message)
            workflow.add_node("execute_tool", execute_tool)
            workflow.add_node("evaluate", evaluate_result)

            # Add edges with conditional routing
            workflow.add_edge("process", "execute_tool")
            workflow.add_edge("execute_tool", "evaluate")
            workflow.add_edge("evaluate", "process")

            # Add end condition
            workflow.add_conditional_edges(
                "evaluate",
                should_end,
                {
                    True: END,
                    False: "process"
                }
            )

            return workflow.compile()

        async def execute_tool(state: AgentState) -> AgentState:
            """Execute the selected tool.

            Args:
                state: Current agent state

            Returns:
                Updated agent state
            """
            # Tool execution logic
            tool_name = state["next_step"]
            tool = next(t for t in state["tools"] if t.name == tool_name)
            result = await tool.ainvoke(state["messages"][-1].content)

            # Update state with tool result
            state["messages"].append(
                AIMessage(content=f"Tool {tool_name} result: {result}")
            )
            return state
        ]]>
      </example>

      <example name="parallel_agents">
        <![CDATA[
        from typing import Dict, List, TypedDict, Any
        from langchain_core.messages import BaseMessage, HumanMessage
        from langgraph.graph import StateGraph
        from operator import itemgetter
        from langchain_core.runnables import RunnablePassthrough

        class ParallelState(TypedDict):
            """Type definition for parallel processing state."""
            tasks: List[Dict[str, Any]]
            results: List[Dict[str, Any]]
            errors: List[Dict[str, str]]
            memory: Dict[str, Any]

        async def create_parallel_workflow(
            num_agents: int = 3,
            model: ChatOpenAI = None
        ) -> StateGraph:
            """Create a workflow for parallel agent processing.

            Args:
                num_agents: Number of parallel agents to use
                model: Language model to use

            Returns:
                StateGraph: A configured parallel workflow
            """
            # Initialize workflow
            workflow = StateGraph(ParallelState)

            # Create agent nodes
            for i in range(num_agents):
                workflow.add_node(
                    f"agent_{i}",
                    create_agent_node(i, model)
                )

            # Add aggregator
            workflow.add_node(
                "aggregate",
                aggregate_results
            )

            # Configure parallel execution
            workflow.add_parallel_edges(
                [f"agent_{i}" for i in range(num_agents)],
                "aggregate"
            )

            # Add end condition
            workflow.add_conditional_edges(
                "aggregate",
                should_end,
                {
                    True: END,
                    False: "agent_0"  # Return to first agent if needed
                }
            )

            return workflow.compile()

        async def create_agent_node(
            agent_id: int,
            model: ChatOpenAI
        ) -> Callable[[ParallelState], ParallelState]:
            """Create an agent processing node.

            Args:
                agent_id: Agent identifier
                model: Language model

            Returns:
                Agent processing function
            """
            async def agent_fn(state: ParallelState) -> ParallelState:
                try:
                    # Process assigned task
                    task = state["tasks"][agent_id]
                    response = await model.ainvoke([
                        HumanMessage(content=task["input"])
                    ])

                    # Store result
                    state["results"].append({
                        "agent_id": agent_id,
                        "task_id": task["id"],
                        "result": response.content
                    })
                except Exception as e:
                    # Handle errors
                    state["errors"].append({
                        "agent_id": agent_id,
                        "error": str(e)
                    })
                return state

            return agent_fn

        async def aggregate_results(state: ParallelState) -> ParallelState:
            """Aggregate results from parallel processing.

            Args:
                state: Current parallel processing state

            Returns:
                Updated state with aggregated results
            """
            # Combine results from all agents
            combined_results = []
            for result in state["results"]:
                if not any(e["agent_id"] == result["agent_id"]
                          for e in state["errors"]):
                    combined_results.append(result)

            # Update state
            state["memory"]["last_results"] = combined_results
            state["results"] = []  # Clear for next iteration
            state["errors"] = []   # Clear errors

            return state
        ]]>
      </example>
    </langgraph_examples>

    <best_practices>
      <standard>Always implement proper error handling and retries for LLM calls.</standard>
      <standard>Use streaming responses for better user experience with long-running operations.</standard>
      <standard>Implement proper memory management for conversational contexts.</standard>
      <standard>Structure complex workflows using LangGraph for better maintainability.</standard>
      <standard>Use typed dictionaries and proper annotations for all agent states and functions.</standard>
      <standard>Implement proper logging and monitoring for AI/ML operations.</standard>
      <standard>Handle rate limits and quotas through configurable retry mechanisms.</standard>
      <standard>Use async operations consistently throughout AI/ML integrations.</standard>
    </best_practices>
  </ai_integration_standards>

  <code_quality>
    <standard>Add typing annotations to ALL functions and classes with return types.</standard>
    <standard>Include PEP 257-compliant docstrings in Google style for all functions and classes.</standard>
    <standard>Implement robust error handling and logging using structlog with context capture.</standard>
    <standard>Use descriptive variable and function names.</standard>
    <standard>Add detailed comments for complex logic.</standard>
    <standard>Provide rich error context for debugging.</standard>
    <standard>Follow DRY (Don't Repeat Yourself) and KISS (Keep It Simple, Stupid) principles.</standard>
    <standard>Use dataclasses for configuration and structured data.</standard>
    <standard>Implement proper error boundaries and exception handling.</standard>
    <standard>Use structlog for all logging with proper formatting and context.</standard>
    <standard>Return only modified portions of code in generation results to optimize token usage.</standard>
    <standard>Use aiofiles for all async file operations instead of builtin open, with appropriate file modes specified.</standard>
    <standard>In async functions, NEVER use the built-in `open()` function - always use `aiofiles.open()` to prevent blocking I/O operations.</standard>

    <error_handling>
      <standard>Define custom exception hierarchies for domain-specific errors.</standard>
      <standard>Use contextual error messages with structured data.</standard>
      <standard>Implement proper error recovery strategies.</standard>
      <standard>Add error boundaries at appropriate system boundaries.</standard>
      <example name="exception_hierarchy">
        <![CDATA[
        from typing import Optional, Dict, Any
        from dataclasses import dataclass

        class ContextForgeError(Exception):
            """Base exception class for ContextForge errors."""
            pass

        class ConfigurationError(ContextForgeError):
            """Raised when there is a configuration-related error."""
            pass

        class AIError(ContextForgeError):
            """Base class for AI-related errors."""
            pass

        @dataclass
        class LLMError(AIError):
            """Error raised during LLM operations.

            Attributes:
                message: Error message
                model: Name of the LLM model
                prompt: Input prompt that caused the error
                response: Optional response from the model
                context: Additional error context
            """
            message: str
            model: str
            prompt: str
            response: Optional[str] = None
            context: Optional[Dict[str, Any]] = None

            def __str__(self) -> str:
                """Get string representation of the error.

                Returns:
                    str: Formatted error message
                """
                parts = [f"LLM Error ({self.model}): {self.message}"]
                if self.response:
                    parts.append(f"Response: {self.response}")
                if self.context:
                    parts.append(f"Context: {self.context}")
                return "\n".join(parts)

        def handle_llm_error(error: Exception, context: Dict[str, Any]) -> None:
            """Handle LLM-related errors with proper logging and recovery.

            Args:
                error: The caught exception
                context: Error context

            Raises:
                LLMError: Wrapped error with context
            """
            logger.error(
                "LLM operation failed",
                error=str(error),
                **context
            )
            raise LLMError(
                message=str(error),
                model=context.get("model", "unknown"),
                prompt=context.get("prompt", ""),
                context=context
            )
        ]]>
      </example>

      <example name="error_boundary">
        <![CDATA[
        from typing import TypeVar, Callable, Awaitable, ParamSpec
        from functools import wraps
        import structlog

        T = TypeVar("T")
        P = ParamSpec("P")

        def error_boundary(
            logger: structlog.BoundLogger
        ) -> Callable[[Callable[P, Awaitable[T]]], Callable[P, Awaitable[T]]]:
            """Create an error boundary for async functions.

            Args:
                logger: Structured logger instance

            Returns:
                Decorator function for error handling
            """
            def decorator(
                func: Callable[P, Awaitable[T]]
            ) -> Callable[P, Awaitable[T]]:
                @wraps(func)
                async def wrapper(*args: P.args, **kwargs: P.kwargs) -> T:
                    try:
                        return await func(*args, **kwargs)
                    except ConfigurationError as e:
                        logger.error(
                            "Configuration error",
                            error=str(e),
                            function=func.__name__
                        )
                        raise
                    except LLMError as e:
                        logger.error(
                            "LLM error",
                            error=str(e),
                            model=e.model,
                            function=func.__name__
                        )
                        raise
                    except Exception as e:
                        logger.exception(
                            "Unexpected error",
                            error=str(e),
                            function=func.__name__
                        )
                        raise
                return wrapper
            return decorator

        # Usage example
        logger = structlog.get_logger()

        @error_boundary(logger)
        async def process_document(content: str) -> Dict[str, Any]:
            """Process a document with error handling.

            Args:
                content: Document content to process

            Returns:
                Dict[str, Any]: Processing results

            Raises:
                LLMError: If LLM processing fails
                ConfigurationError: If configuration is invalid
            """
            # Implementation
            pass
        ]]>
      </example>
    </error_handling>

    <logging_patterns>
      <standard>Use structlog for all logging operations.</standard>
      <standard>Include contextual data in log events.</standard>
      <standard>Use proper log levels consistently.</standard>
      <standard>Add correlation IDs for request tracing.</standard>
      <example name="structured_logging">
        <![CDATA[
        from typing import Any, Dict, Optional
        import structlog
        from uuid import uuid4

        def configure_logging() -> None:
            """Configure structlog for the application.

            This sets up structured logging with proper processors
            and output formatting.
            """
            structlog.configure(
                processors=[
                    structlog.contextvars.merge_contextvars,
                    structlog.processors.add_log_level,
                    structlog.processors.TimeStamper(fmt="iso"),
                    structlog.processors.StackInfoRenderer(),
                    structlog.processors.format_exc_info,
                    structlog.processors.UnicodeDecoder(),
                    structlog.processors.JSONRenderer()
                ],
                wrapper_class=structlog.make_filtering_bound_logger(
                    structlog.get_config()["min_level"]
                ),
                context_class=dict,
                logger_factory=structlog.PrintLoggerFactory(),
                cache_logger_on_first_use=True
            )

        class RequestContext:
            """Context manager for request-scoped logging."""

            def __init__(
                self,
                correlation_id: Optional[str] = None,
                **kwargs: Any
            ) -> None:
                """Initialize request context.

                Args:
                    correlation_id: Optional correlation ID
                    **kwargs: Additional context variables
                """
                self.correlation_id = correlation_id or str(uuid4())
                self.context = {
                    "correlation_id": self.correlation_id,
                    **kwargs
                }
                self.logger = structlog.get_logger()

            async def __aenter__(self) -> structlog.BoundLogger:
                """Enter the context and bind variables.

                Returns:
                    structlog.BoundLogger: Configured logger
                """
                self.logger = self.logger.bind(**self.context)
                self.logger.info("Request started", **self.context)
                return self.logger

            async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
                """Exit the context and log completion.

                Args:
                    exc_type: Exception type if raised
                    exc_val: Exception value if raised
                    exc_tb: Exception traceback if raised
                """
                if exc_val:
                    self.logger.error(
                        "Request failed",
                        error=str(exc_val),
                        error_type=exc_type.__name__
                    )
                else:
                    self.logger.info("Request completed")

        # Usage example
        async def process_request(data: Dict[str, Any]) -> None:
            """Process a request with structured logging.

            Args:
                data: Request data to process
            """
            async with RequestContext(
                correlation_id=data.get("request_id"),
                user_id=data.get("user_id"),
                operation=data.get("operation")
            ) as logger:
                logger.info("Processing request", data=data)
                # Process request
                logger.info("Request processed successfully")
        ]]>
      </example>
    </logging_patterns>

    <type_safety>
      <standard>Use proper type annotations for all code.</standard>
      <standard>Leverage TypeVar and ParamSpec for generic types.</standard>
      <standard>Use Literal types for constrained values.</standard>
      <standard>Add runtime type checking where necessary.</standard>
      <example name="type_patterns">
        <![CDATA[
        from typing import (
            TypeVar, ParamSpec, Callable, Awaitable,
            Dict, List, Optional, Union, Literal,
            Protocol, runtime_checkable
        )
        from dataclasses import dataclass
        from enum import Enum

        # Type variables
        T = TypeVar("T")
        P = ParamSpec("P")

        # Enums and Literals
        class LogLevel(str, Enum):
            """Log level enumeration."""
            DEBUG = "debug"
            INFO = "info"
            WARNING = "warning"
            ERROR = "error"

        MessageType = Literal["text", "image", "file"]

        # Protocols
        @runtime_checkable
        class Processable(Protocol):
            """Protocol for processable items."""

            async def process(self) -> Dict[str, Any]:
                """Process the item.

                Returns:
                    Dict[str, Any]: Processing results
                """
                ...

        # Dataclasses with types
        @dataclass
        class ProcessingResult:
            """Result of processing operation.

            Attributes:
                success: Whether processing succeeded
                data: Processed data
                errors: Optional error messages
            """
            success: bool
            data: Dict[str, Any]
            errors: Optional[List[str]] = None

        # Generic function types
        AsyncProcessor = Callable[[T], Awaitable[ProcessingResult]]

        # Type-safe function
        async def process_items(
            items: List[T],
            processor: AsyncProcessor[T]
        ) -> List[ProcessingResult]:
            """Process multiple items with type safety.

            Args:
                items: Items to process
                processor: Processing function

            Returns:
                List[ProcessingResult]: Processing results
            """
            results: List[ProcessingResult] = []
            for item in items:
                result = await processor(item)
                results.append(result)
            return results

        # Runtime type checking
        def ensure_type(value: Any, expected_type: type) -> None:
            """Ensure a value matches an expected type.

            Args:
                value: Value to check
                expected_type: Expected type

            Raises:
                TypeError: If type doesn't match
            """
            if not isinstance(value, expected_type):
                raise TypeError(
                    f"Expected {expected_type.__name__}, got {type(value).__name__}"
                )
        ]]>
      </example>
    </type_safety>

    <documentation_standards>
      <standard>Use Google-style docstrings for all code.</standard>
      <standard>Include type information in docstrings.</standard>
      <standard>Document exceptions and error conditions.</standard>
      <standard>Add usage examples for complex functions.</standard>
      <example name="docstring_patterns">
        <![CDATA[
        from typing import Dict, List, Optional, Union, Any
        import asyncio

        class DocumentProcessor:
            """Process documents with various transformations.

            This class provides functionality to process documents through
            a pipeline of transformations, with proper error handling and
            logging.

            Attributes:
                max_retries: Maximum number of retry attempts
                timeout: Operation timeout in seconds

            Example:
                ```python
                processor = DocumentProcessor(max_retries=3)
                result = await processor.process_document({
                    "content": "Example content",
                    "type": "text"
                })
                ```
            """

            def __init__(
                self,
                max_retries: int = 3,
                timeout: float = 30.0
            ) -> None:
                """Initialize the document processor.

                Args:
                    max_retries: Maximum number of retry attempts
                    timeout: Operation timeout in seconds

                Raises:
                    ValueError: If max_retries is negative
                """
                if max_retries < 0:
                    raise ValueError("max_retries must be non-negative")
                self.max_retries = max_retries
                self.timeout = timeout
                self.logger = structlog.get_logger()

            async def process_document(
                self,
                document: Dict[str, Any],
                *,
                priority: Optional[int] = None
            ) -> Dict[str, Any]:
                """Process a document through the transformation pipeline.

                This method applies a series of transformations to the document,
                with proper error handling and retries.

                Args:
                    document: Document to process
                    priority: Optional processing priority (higher = more urgent)

                Returns:
                    Dict[str, Any]: Processed document

                Raises:
                    ProcessingError: If processing fails after retries
                    ValueError: If document is invalid
                    TimeoutError: If processing exceeds timeout

                Example:
                    ```python
                    doc = {
                        "content": "Example content",
                        "type": "text"
                    }
                    result = await processor.process_document(
                        doc,
                        priority=1
                    )
                    ```
                """
                self.logger.info(
                    "Processing document",
                    document_type=document.get("type"),
                    priority=priority
                )

                try:
                    async with asyncio.timeout(self.timeout):
                        result = await self._process_with_retries(
                            document,
                            priority=priority
                        )
                    return result
                except asyncio.TimeoutError as e:
                    self.logger.error(
                        "Document processing timed out",
                        timeout=self.timeout
                    )
                    raise TimeoutError(
                        f"Processing timed out after {self.timeout}s"
                    ) from e
                except Exception as e:
                    self.logger.exception("Document processing failed")
                    raise
        ]]>
      </example>
    </documentation_standards>

    <structlog_standards>
      <standard>Use structlog for all logging operations with proper configuration.</standard>
      <standard>Configure structlog at application startup with appropriate processors.</standard>
      <standard>Use bound loggers with context for all logging operations.</standard>
      <standard>Implement proper correlation ID tracking across requests.</standard>
      <standard>Add appropriate log levels and maintain consistency.</standard>
      <standard>Include relevant context in all log messages.</standard>
      <standard>Use proper error logging with exception information.</standard>
      <standard>Configure JSON output format for production environments.</standard>

      <configuration>
        <standard>Configure structlog at application startup</standard>
        <example name="base_configuration">
          <![CDATA[
          from typing import Any, Dict, Optional
          import structlog
          from structlog.types import Processor
          import logging
          import sys
          from rich.console import Console

          def configure_logging(
              log_level: str = "INFO",
              environment: str = "development",
              json_format: bool = False
          ) -> None:
              """Configure structlog for the application.

              This sets up structured logging with appropriate processors
              and formatters based on the environment.

              Args:
                  log_level: Minimum log level to record
                  environment: Current environment (development/production)
                  json_format: Whether to use JSON output format
              """
              console = Console(force_terminal=True)

              # Set up common processors
              processors: list[Processor] = [
                  # Add timestamps in ISO format
                  structlog.processors.TimeStamper(fmt="iso"),

                  # Add log level name
                  structlog.processors.add_log_level,

                  # Add caller information
                  structlog.processors.CallsiteParameterAdder(
                      parameters={
                          "func_name": "func_name",
                          "lineno": "lineno",
                          "module": "module"
                      }
                  ),

                  # Add thread information for concurrent operations
                  structlog.processors.ThreadProcessor(),

                  # Add process information
                  structlog.processors.ProcessorFormatter.wrap_for_formatter,

                  # Merge context variables
                  structlog.contextvars.merge_contextvars,

                  # Handle exceptions properly
                  structlog.processors.format_exc_info,

                  # Handle key names consistently
                  structlog.processors.KeyValueRenderer()
              ]

              # Add environment-specific processors
              if environment == "development":
                  processors.extend([
                      structlog.dev.ConsoleRenderer(
                          colors=True,
                          exception_formatter=structlog.dev.plain_traceback
                      )
                  ])
              else:
                  processors.extend([
                      structlog.processors.dict_tracebacks,
                      structlog.processors.JSONRenderer()
                  ])

              # Configure structlog
              structlog.configure(
                  processors=processors,
                  context_class=dict,
                  logger_factory=structlog.PrintLoggerFactory(),
                  wrapper_class=structlog.make_filtering_bound_logger(
                      logging.getLevelName(log_level)
                  ),
                  cache_logger_on_first_use=True
              )

              # Configure standard logging if needed
              logging.basicConfig(
                  format="%(message)s",
                  stream=sys.stdout,
                  level=log_level
              )
          ]]>
        </example>

        <example name="context_management">
          <![CDATA[
          from typing import Any, AsyncGenerator, Optional
          from contextlib import asynccontextmanager
          import structlog
          from uuid import uuid4

          class RequestContext:
              """Context manager for request-scoped logging."""

              def __init__(
                  self,
                  correlation_id: Optional[str] = None,
                  **kwargs: Any
              ) -> None:
                  """Initialize request context.

                  Args:
                      correlation_id: Optional correlation ID
                      **kwargs: Additional context variables
                  """
                  self.correlation_id = correlation_id or str(uuid4())
                  self.context = {
                      "correlation_id": self.correlation_id,
                      **kwargs
                  }
                  self.logger = structlog.get_logger()

              async def __aenter__(self) -> structlog.BoundLogger:
                  """Enter the context and bind variables.

                  Returns:
                      structlog.BoundLogger: Configured logger
                  """
                  self.logger = self.logger.bind(**self.context)
                  self.logger.info("Request started")
                  return self.logger

              async def __aexit__(
                  self,
                  exc_type: Optional[type[BaseException]],
                  exc_val: Optional[BaseException],
                  exc_tb: Optional[Any]
              ) -> None:
                  """Exit the context and log completion.

                  Args:
                      exc_type: Exception type if raised
                      exc_val: Exception value if raised
                      exc_tb: Exception traceback if raised
                  """
                  if exc_val:
                      self.logger.error(
                          "Request failed",
                          error=str(exc_val),
                          error_type=exc_type.__name__ if exc_type else None
                      )
                  else:
                      self.logger.info("Request completed")

          @asynccontextmanager
          async def log_operation(
              operation: str,
              **context: Any
          ) -> AsyncGenerator[structlog.BoundLogger, None]:
              """Context manager for logging operations.

              Args:
                  operation: Operation name
                  **context: Additional context variables

              Yields:
                  structlog.BoundLogger: Configured logger
              """
              logger = structlog.get_logger().bind(
                  operation=operation,
                  **context
              )
              try:
                  logger.info(f"{operation} started")
                  yield logger
              except Exception as e:
                  logger.exception(f"{operation} failed", error=str(e))
                  raise
              else:
                  logger.info(f"{operation} completed")
          ]]>
        </example>
      </configuration>

      <usage_patterns>
        <standard>Use bound loggers with context</standard>
        <standard>Include relevant context in all log messages</standard>
        <standard>Use proper error logging with exception information</standard>
        <example name="logging_patterns">
          <![CDATA[
          import structlog
          from typing import Any, Dict, Optional

          # Initialize logger
          logger = structlog.get_logger()

          async def process_document(
              document_id: str,
              content: str,
              **kwargs: Any
          ) -> Dict[str, Any]:
              """Process a document with proper logging.

              Args:
                  document_id: Document identifier
                  content: Document content
                  **kwargs: Additional processing parameters

              Returns:
                  Dict[str, Any]: Processing results

              Raises:
                  ProcessingError: If processing fails
              """
              # Bind context to logger
              log = logger.bind(
                  document_id=document_id,
                  content_length=len(content),
                  **kwargs
              )

              try:
                  log.info("Starting document processing")

                  # Process document
                  result = await perform_processing(content)

                  log.info(
                      "Document processing complete",
                      result_size=len(result)
                  )
                  return result

              except Exception as e:
                  # Log error with full context
                  log.exception(
                      "Document processing failed",
                      error=str(e),
                      error_type=type(e).__name__
                  )
                  raise ProcessingError(
                      f"Failed to process document {document_id}"
                  ) from e

          async def handle_request(
              request_id: str,
              user_id: str,
              operation: str,
              **params: Any
          ) -> Any:
              """Handle a request with proper logging context.

              Args:
                  request_id: Request identifier
                  user_id: User identifier
                  operation: Operation name
                  **params: Operation parameters

              Returns:
                  Any: Operation result
              """
              async with RequestContext(
                  correlation_id=request_id,
                  user_id=user_id,
                  operation=operation,
                  **params
              ) as log:
                  # Log request parameters
                  log.info(
                      "Processing request",
                      params=params
                  )

                  # Perform operation
                  result = await perform_operation(
                      operation,
                      **params
                  )

                  # Log success with relevant metrics
                  log.info(
                      "Request processed",
                      result_type=type(result).__name__,
                      metrics={
                          "processing_time": 1.23,
                          "result_size": len(str(result))
                      }
                  )

                  return result
          ]]>
        </example>
      </usage_patterns>

      <best_practices>
        <standard>Always use bound loggers with context</standard>
        <standard>Include correlation IDs for request tracking</standard>
        <standard>Use proper log levels consistently</standard>
        <standard>Add context managers for operation logging</standard>
        <standard>Configure appropriate processors for each environment</standard>
        <standard>Use JSON format in production</standard>
        <standard>Include relevant metrics and measurements</standard>
        <standard>Handle sensitive data appropriately</standard>
        <standard>Use proper error logging with full context</standard>
        <standard>Implement request-scoped logging</standard>
        <standard>Add caller information when helpful</standard>
        <standard>Use proper exception formatting</standard>
      </best_practices>

      <testing>
        <standard>Use structlog's testing.capture_logs for testing</standard>
        <standard>Never use pytest's caplog for structlog testing</standard>
        <standard>Test log messages using event checking</standard>
        <example name="log_testing">
          <![CDATA[
          import pytest
          import structlog
          from typing import Generator, Any

          @pytest.fixture
          def capture_logs() -> Generator[Any, None, None]:
              """Fixture for capturing structlog output.

              Yields:
                  List of captured log entries
              """
              with structlog.testing.capture_logs() as captured:
                  yield captured

          async def test_document_processing(
              capture_logs: Any,
              sample_document: Dict[str, Any]
          ) -> None:
              """Test document processing with log verification.

              Args:
                  capture_logs: Log capture fixture
                  sample_document: Test document fixture
              """
              # Process document
              await process_document(
                  document_id=sample_document["id"],
                  content=sample_document["content"]
              )

              # Verify log messages
              assert any(
                  log.get("event") == "Starting document processing"
                  for log in capture_logs
              ), "Missing start log message"

              assert any(
                  log.get("event") == "Document processing complete"
                  for log in capture_logs
              ), "Missing completion log message"

              # Verify context
              assert any(
                  log.get("document_id") == sample_document["id"]
                  for log in capture_logs
              ), "Missing document ID in log context"
          ]]>
        </example>
      </testing>
    </structlog_standards>
  </code_quality>

  <import_standards>
    <standard>Always use Ruff's isort (I) rules for import sorting.</standard>
    <standard>Place `from __future__ import annotations` at the top of every Python file.</standard>
    <standard>Group imports into sections separated by a blank line:
      1. Future imports
      2. Standard library imports
      3. Third-party imports
      4. Local imports from contextforge_cli
      5. Type checking imports (if needed)
    </standard>
    <standard>Sort imports alphabetically within each section.</standard>
    <standard>Use absolute imports instead of relative imports.</standard>
    <standard>Add appropriate linter directives at the top of files based on imports:
      - For Discord.py files:
        ```python
        # pylint: disable=no-member
        # pyright: reportAttributeAccessIssue=false
        ```
      - For Pydantic files:
        ```python
        # pylint: disable=no-name-in-module
        # pylint: disable=no-member
        # pyright: reportInvalidTypeForm=false
        # pyright: reportUndefinedVariable=false
        ```
      - For files using both:
        ```python
        # pylint: disable=no-name-in-module
        # pylint: disable=no-member
        # pyright: reportInvalidTypeForm=false
        # pyright: reportUndefinedVariable=false
        # pyright: reportAttributeAccessIssue=false
        ```
    </standard>

    <test_imports>
      <standard>For test files, always import TYPE_CHECKING block with pytest types:
        ```python
        if TYPE_CHECKING:
            from _pytest.capture import CaptureFixture
            from _pytest.fixtures import FixtureRequest
            from _pytest.logging import LogCaptureFixture
            from _pytest.monkeypatch import MonkeyPatch
            from pytest_mock.plugin import MockerFixture
        ```
      </standard>
      <standard>Group pytest-specific imports together.</standard>
      <standard>Import test utilities from conftest.py when shared across tests.</standard>
    </test_imports>

    <import_organization>
      <standard>Use consistent import aliases across the codebase.</standard>
      <standard>Import only what is needed, avoid wildcard imports (*).</standard>
      <standard>Use multi-line imports for better readability when importing multiple items.</standard>
      <standard>Keep import blocks organized and clean with proper spacing.</standard>
      <example>
        <![CDATA[
        # Good import organization
        from __future__ import annotations

        import asyncio
        import os
        import sys
        from pathlib import Path
        from typing import TYPE_CHECKING, Any, Optional

        import structlog
        import typer
        from rich import print as rprint
        from rich.console import Console

        from contextforge_cli.bot_logger import configure_logging
        from contextforge_cli.shell import ProcessException, ShellConsole

        if TYPE_CHECKING:
            from _pytest.capture import CaptureFixture
            from _pytest.monkeypatch import MonkeyPatch
        ]]>
      </example>
    </import_organization>

    <import_patterns>
      <standard>Use consistent patterns for commonly used imports:
        - Type imports: `from typing import TYPE_CHECKING, Any, Optional`
        - Path handling: `from pathlib import Path`
        - Async operations: `import asyncio`
        - Logging: `import structlog`
        - CLI: `import typer`
        - Console output: `from rich import print as rprint`
      </standard>
      <standard>Use type annotation imports consistently:
        - Collections: `from collections.abc import AsyncGenerator, Generator`
        - Typing: `from typing import TypeVar, Union`
      </standard>
    </import_patterns>

    <best_practices>
      <standard>Always add type checking imports in a TYPE_CHECKING block.</standard>
      <standard>Use consistent import aliases across the codebase (e.g., `rprint` for `rich.print`).</standard>
      <standard>Keep imports organized and clean with proper spacing.</standard>
      <standard>Import only what is needed to minimize namespace pollution.</standard>
      <standard>Use multi-line imports for better readability when importing multiple items.</standard>
      <standard>Add appropriate linter directives based on imported packages.</standard>
    </best_practices>

    <configuration>
      <standard>Configure isort in pyproject.toml:
        ```toml
        [tool.isort]
        profile = "black"
        known_first_party = ["contextforge_cli"]
        sections = ["FUTURE", "STDLIB", "THIRDPARTY", "FIRSTPARTY", "LOCALFOLDER"]
        default_section = "THIRDPARTY"
        multi_line_output = 3
        include_trailing_comma = true
        force_grid_wrap = 0
        use_parentheses = true
        ensure_newline_before_comments = true
        line_length = 88
        ```
      </standard>
    </configuration>
  </import_standards>

  <testing>
    <standard>Use pytest exclusively for all testing (no unittest module).</standard>
    <standard>Place all tests in `./tests/` directory with proper subdirectories:
      - tests/unittests/: Unit tests
      - tests/integration/: Integration tests
      - tests/conftest.py: Shared fixtures
    </standard>
    <standard>Include `__init__.py` files in all test directories and subdirectories.</standard>
    <standard>Add type annotations and docstrings to all tests.</standard>
    <standard>Use pytest markers to categorize tests (e.g., `@pytest.mark.unit`, `@pytest.mark.integration`, `@pytest.mark.asyncio`).</standard>
    <standard>Mark cursor-generated code with `@pytest.mark.cursor`.</standard>
    <standard>Strive for 100% unit test code coverage.</standard>
    <standard>Use pytest-recording for tests involving external API calls.</standard>
    <standard>Use typer.testing.CliRunner for CLI application testing.</standard>
    <standard>For file-based tests, use tmp_path fixture to handle test files.</standard>
    <standard>Avoid context managers for pytest mocks, use mocker.patch instead.</standard>
    <standard>Mirror source code directory structure in tests directory.</standard>
    <standard>Use VCR.py for recording and replaying HTTP interactions in tests.</standard>

    <test_execution>
      <standard>When encountering a broken test suite, always fix tests one at a time:</standard>
      <standard>1. Run individual tests in isolation to identify and fix each failure</standard>
      <standard>2. Only run the full test suite after all individual tests pass</standard>
      <standard>3. Use specific test selection to target individual tests</standard>
      <example>
        <![CDATA[
        # Run a specific test file with verbose output and local variables shown
        uv run pytest -s --verbose --showlocals --tb=short path/to/file.py

        # Run a specific test function within a file
        uv run pytest -s --verbose --showlocals --tb=short path/to/file.py::test_function_name

        # Run a specific test in a class
        uv run pytest -s --verbose --showlocals --tb=short path/to/file.py::TestClass::test_method_name

        # Examples:
        uv run pytest -s --verbose --showlocals --tb=short tests/test_logsetup.py
        uv run pytest -s --verbose --showlocals --tb=short tests/test_bot.py::test_ping_command
        uv run pytest -s --verbose --showlocals --tb=short tests/test_bot.py::TestCommands::test_help_command

        # After fixing individual tests, run the full suite
        uv run pytest
        ]]>
      </example>
    </test_execution>

    <directory_structure>
      <standard>Organize tests into logical subdirectories matching source code structure:</standard>
      <structure>
        <![CDATA[
        tests/
         __init__.py
         conftest.py              # Global test fixtures and configuration
         fake_embeddings.py       # Test utilities for embeddings
         test_*.py               # Top-level tests (e.g., test_shell.py, test_logsetup.py)
         integration/            # Integration tests
            __init__.py
         unittests/             # Unit tests matching source structure
             __init__.py
        ]]>
      </structure>
      <conventions>
        <standard>Place all test files in appropriate directories based on test type</standard>
        <standard>Use `test_` prefix for all test files</standard>
        <standard>Keep test utilities at the top level if they are used across multiple test types</standard>
        <standard>Mirror the source code package structure in unittests/ directory</standard>
        <standard>Place integration tests that test multiple components together in integration/ directory</standard>
        <standard>Use conftest.py for shared fixtures and configurations</standard>
      </conventions>
    </directory_structure>

    <test_types>
      <standard>Unit tests should be placed in tests/unittests/ directory</standard>
      <standard>Integration tests should be placed in tests/integration/ directory</standard>
      <standard>End-to-end tests should be placed in tests/e2e/ directory (if needed)</standard>
      <standard>Performance tests should be placed in tests/performance/ directory (if needed)</standard>
      <standard>Top-level tests should be used for core functionality that doesn't fit into other categories</standard>
    </test_types>

    <test_fixtures>
      <standard>Define shared fixtures in conftest.py files</standard>
      <standard>Use proper typing for all fixtures</standard>
      <standard>Include comprehensive docstrings for all fixtures</standard>
      <standard>Use appropriate fixture scopes (function, class, module, session)</standard>
    </test_fixtures>
  </testing>

  <dependency_management>
    <standard>Use UV (https://docs.astral.sh/uv) as the primary dependency management tool.</standard>
    <standard>Use `uv sync` for dependency installation, avoid `uv pip install`.</standard>
    <standard>Maintain lock file consistency with `uv lock --check`.</standard>
    <standard>Use `uv.lock` for deterministic dependency resolution.</standard>
    <standard>Document all dependencies in pyproject.toml with specific version constraints.</standard>

    <version_management>
      <standard>Use >= version constraints for dependencies to allow compatible updates.</standard>
      <standard>Pin exact versions in uv.lock for reproducible builds.</standard>
      <standard>Use `uv lock --upgrade` for controlled dependency updates.</standard>
      <standard>Run `uv lock --check` before commits to ensure lock file consistency.</standard>
    </version_management>

    <dependency_groups>
      <standard>Organize dependencies into logical groups in pyproject.toml:</standard>
      <groups>
        <group>
          <name>project.dependencies</name>
          <description>Core runtime dependencies required for the application</description>
        </group>
        <group>
          <name>dependency-groups.dev</name>
          <description>Development dependencies (testing, linting, type checking)</description>
        </group>
      </groups>
    </dependency_groups>

    <workflows>
      <standard>Follow these workflows for dependency management:</standard>
      <workflow>
        <name>Installation</name>
        <steps>
          <step>Run `uv sync` to install dependencies from lock file</step>
          <step>Use `uv sync --all-extras --dev` for development environment</step>
        </steps>
      </workflow>
      <workflow>
        <name>Updating Dependencies</name>
        <steps>
          <step>Run `uv lock --upgrade --dry-run` to preview updates</step>
          <step>Run `uv lock --upgrade` to update dependencies</step>
          <step>Run `uv sync --all-extras --dev` to apply updates</step>
        </steps>
      </workflow>
      <workflow>
        <name>Version Management</name>
        <steps>
          <step>Use `uv version` to check current version</step>
          <step>Use `uv version --bump [major|minor|patch]` for version updates</step>
        </steps>
      </workflow>
    </workflows>

    <tool_integration>
      <standard>Configure UV with other development tools:</standard>
      <integrations>
        <integration>
          <tool>pre-commit</tool>
          <config>Use `uv run pre-commit install` for hook installation</config>
        </integration>
        <integration>
          <tool>pytest</tool>
          <config>Use `uv run pytest` for test execution</config>
        </integration>
        <integration>
          <tool>mypy</tool>
          <config>Use `uv run mypy` for type checking</config>
        </integration>
      </integrations>
    </tool_integration>

    <best_practices>
      <standard>Always run `uv lock --check` before committing changes to ensure lock file consistency</standard>
      <standard>Use `uv sync --all-extras --dev` for complete development environment setup</standard>
      <standard>Document dependency changes in commit messages</standard>
      <standard>Keep dependencies organized by purpose in pyproject.toml</standard>
      <standard>Use dependency groups to manage optional features</standard>
      <standard>Regularly update dependencies with `uv lock --upgrade`</standard>
      <standard>Run test suite after dependency updates</standard>
    </best_practices>

    <example>
      <![CDATA[
      # Example pyproject.toml dependency specification
      [project]
      dependencies = [
          "langchain>=0.3.17",
          "langchain-core>=0.3.33",
          "langgraph>=0.2.69",
          "structlog>=24.4.0",
          "typing-extensions>=4.12.2"
      ]

      [dependency-groups]
      dev = [
          "pytest>=8.3.4",
          "pytest-asyncio>=0.25.3",
          "pytest-cov>=6.0.0",
          "ruff>=0.9.4",
          "mypy>=1.8.0"
      ]

      # Example UV commands
      $ uv sync --all-extras --dev  # Install all dependencies
      $ uv lock --check            # Verify lock file consistency
      $ uv lock --upgrade         # Update dependencies
      $ uv version --bump minor   # Bump version number
      ]]>
    </example>
  </dependency_management>

  <langchain_standards>
    <standard>Mark tests involving Langchain runnables with @pytest.mark.vcr (except evaluation tests).</standard>
    <standard>Use proper VCR.py configuration for HTTP interaction recording.</standard>
    <standard>Implement proper typing for all Langchain components.</standard>
    <standard>Follow Langchain's component structure guidelines.</standard>
    <standard>Create distinct files for different LangChain component types.</standard>
    <standard>Use proper error handling for LLM API calls.</standard>
    <standard>Implement retry logic for API failures.</standard>
    <standard>Use streaming responses when appropriate.</standard>
    <examples>
      <example>
        <![CDATA[
        # Chain Construction Example
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_openai import ChatOpenAI

        def create_qa_chain(
            model_name: str = "gpt-3.5-turbo",
            temperature: float = 0.7
        ) -> Runnable:
            """Create a question-answering chain.

            Args:
                model_name: Name of the LLM model to use
                temperature: Sampling temperature

            Returns:
                Configured QA chain
            """
            prompt = ChatPromptTemplate.from_template("""
                Answer the question based on the context.
                Context: {context}
                Question: {question}
                Answer:""")

            model = ChatOpenAI(
                model_name=model_name,
                temperature=temperature
            )

            chain = prompt | model | StrOutputParser()
            return chain

        # Error Handling Example
        from tenacity import retry, stop_after_attempt, wait_exponential

        @retry(
            stop=stop_after_attempt(3),
            wait=wait_exponential(multiplier=1, min=4, max=10)
        )
        async def call_llm_with_retry(
            chain: Runnable,
            inputs: Dict[str, Any]
        ) -> str:
            """Call LLM with retry logic.

            Args:
                chain: LangChain runnable
                inputs: Input parameters

            Returns:
                Model response

            Raises:
                Exception: If all retries fail
            """
            try:
                response = await chain.ainvoke(inputs)
                return response
            except Exception as e:
                logger.exception(f"LLM call failed: {str(e)}")
                raise
        ]]>
      </example>
      <example>
        <![CDATA[
        # LangGraph Agent Example
        from langgraph.prebuilt import create_agent_executor
        from langchain_core.messages import HumanMessage
        from typing import Dict, List, Tuple

        async def create_research_agent(
            tools: List[BaseTool],
            system_message: str
        ) -> AgentExecutor:
            """Create a research agent with tools.

            Args:
                tools: List of tools for the agent
                system_message: System prompt for the agent

            Returns:
                Configured agent executor
            """
            agent = create_agent_executor(
                tools=tools,
                llm=ChatOpenAI(temperature=0),
                system_message=system_message
            )

            return agent

        # Agent Usage Example
        async def research_topic(
            agent: AgentExecutor,
            query: str
        ) -> Tuple[str, List[Dict[str, Any]]]:
            """Research a topic using an agent.

            Args:
                agent: Research agent
                query: Research query

            Returns:
                Tuple of final answer and intermediate steps
            """
            result = await agent.ainvoke({
                "input": query,
                "chat_history": []
            })

            return result["output"], result["intermediate_steps"]
        ]]>
      </example>
    </examples>
  </langchain_standards>

  <langgraph_standards>
    <standard>Follow LangGraph's component structure for agent workflows.</standard>
    <standard>Use proper state management in graph nodes.</standard>
    <standard>Implement proper error handling in graph edges.</standard>
    <standard>Use appropriate markers for graph-based tests.</standard>
    <standard>Create reusable graph components when possible.</standard>
  </langgraph_standards>

  <design_patterns>
    <pattern>
      <name>Composition Over Inheritance</name>
      <description>Favor object composition over class inheritance to avoid subclass explosion and enhance flexibility</description>
      <example>
        <![CDATA[
        # Prefer composition
        class DocumentProcessor:
            def __init__(self, loader: BaseLoader, splitter: TextSplitter):
                self.loader = loader
                self.splitter = splitter

        # Instead of inheritance
        class PDFProcessor(BaseLoader, TextSplitter):
            pass
        ]]>
      </example>
    </pattern>
    <pattern>
      <name>Decorator Pattern</name>
      <description>Use for dynamically adjusting behavior of objects without modifying their structure</description>
      <example>
        <![CDATA[
        def log_llm_calls(func: Callable) -> Callable:
            @wraps(func)
            async def wrapper(*args: Any, **kwargs: Any) -> Any:
                logger.info(f"Calling LLM with args: {args}, kwargs: {kwargs}")
                return await func(*args, **kwargs)
            return wrapper
        ]]>
      </example>
    </pattern>
    <pattern>
      <name>Adapter Pattern</name>
      <description>Allow incompatible interfaces to work together, promoting flexibility and reusability</description>
    </pattern>
    <pattern>
      <name>Global Object Pattern</name>
      <description>Use for creating module-level objects that provide methods for actions</description>
    </pattern>
  </design_patterns>

  <configuration_standards>
    <ruff_rules>
      <standard>Document all Ruff rules in pyproject.toml with inline comments and stability indicators.</standard>
      <standard>Use the following stability indicators for each rule:
        -  (stable) - Well-tested rules safe for production
        -  (unstable/preview) - New rules that may change
        -  (deprecated) - Rules that will be removed
        -  (removed) - Rules no longer available
        -  (auto-fixable) - Rules that can be auto-fixed
      </standard>
      <standard>Keep rule descriptions under 160 characters for readability.</standard>
      <standard>Reference Ruff version from .pre-commit-config.yaml.</standard>
      <standard>Group rules by category with clear section comments.</standard>
      <standard>Include rationale for non-default rule selections.</standard>

      <rule_categories>
        <category>
          <name>Error Prevention</name>
          <rules>
            <rule>E401   - Multiple imports on one line</rule>
            <rule>E402  - Module level import not at top of file</rule>
            <rule>E501   - Line too long (82 characters)</rule>
            <rule>E711   - Comparison to None should be 'if cond is None:'</rule>
            <rule>E712   - Comparison to True/False should be 'if cond is True:'</rule>
            <rule>E713   - Test for membership should be 'not in'</rule>
            <rule>E714   - Test for object identity should be 'is not'</rule>
            <rule>E721  - Do not compare types, use 'isinstance()'</rule>
            <rule>E722  - Do not use bare except</rule>
            <rule>E731  - Do not assign a lambda expression, use a def</rule>
          </rules>
        </category>

        <category>
          <name>Code Style</name>
          <rules>
            <rule>D100  - Missing docstring in public module</rule>
            <rule>D101  - Missing docstring in public class</rule>
            <rule>D102  - Missing docstring in public method</rule>
            <rule>D103  - Missing docstring in public function</rule>
            <rule>D104  - Missing docstring in public package</rule>
            <rule>D105  - Missing docstring in magic method</rule>
            <rule>D107  - Missing docstring in __init__</rule>
            <rule>D200   - One-line docstring should fit on one line</rule>
            <rule>D201   - No blank lines allowed before function docstring</rule>
            <rule>D202   - No blank lines allowed after function docstring</rule>
            <rule>D205   - 1 blank line required between summary line and description</rule>
            <rule>D400   - First line should end with a period</rule>
            <rule>D401  - First line should be in imperative mood</rule>
            <rule>D403   - First word of the first line should be capitalized</rule>
            <rule>D415  - First line should end with a period, question mark, or exclamation point</rule>
          </rules>
        </category>

        <category>
          <name>Type Checking</name>
          <rules>
            <rule>ANN001   - Missing type annotation for function argument</rule>
            <rule>ANN002   - Missing type annotation for *args</rule>
            <rule>ANN003   - Missing type annotation for **kwargs</rule>
            <rule>ANN101   - Missing type annotation for self in method</rule>
            <rule>ANN102   - Missing type annotation for cls in classmethod</rule>
            <rule>ANN201   - Missing return type annotation for public function</rule>
            <rule>ANN202   - Missing return type annotation for private function</rule>
            <rule>ANN204   - Missing return type annotation for special method</rule>
            <rule>ANN205   - Missing return type annotation for staticmethod</rule>
            <rule>ANN206   - Missing return type annotation for classmethod</rule>
          </rules>
        </category>

        <category>
          <name>Import Organization</name>
          <rules>
            <rule>I001   - Import block is un-sorted or un-formatted</rule>
            <rule>I002  - Missing required import</rule>
            <rule>I003  - Missing required relative import</rule>
            <rule>I004  - Missing __init__.py</rule>
            <rule>I005   - Import block is un-sorted or un-formatted (nested)</rule>
          </rules>
        </category>
      </rule_categories>

      <configuration_example>
        <![CDATA[
        [tool.ruff]
        # Enable all rules by default, then selectively disable
        select = [
            "E",   # pycodestyle errors (stable)
            "W",   # pycodestyle warnings (stable)
            "F",   # pyflakes (stable)
            "I",   # isort (stable)
            "D",   # pydocstyle (stable)
            "ANN", # flake8-annotations (stable)
        ]

        # Exclude specific rules
        ignore = [
            "D203",  #  Conflicts with D211
            "D213",  #  Conflicts with D212
            "ANN101",  #  Missing type annotation for self
            "ANN102",  #  Missing type annotation for cls
        ]

        # Line length configuration
        line-length = 120

        # Import configuration
        [tool.ruff.isort]
        known-first-party = ["contextforge_cli"]
        section-order = ["future", "standard-library", "third-party", "first-party", "local-folder"]
        combine-as-imports = true

        # Type checking configuration
        [tool.ruff.pyright]
        reportMissingImports = true
        reportMissingTypeStubs = false

        # Docstring configuration
        [tool.ruff.pydocstyle]
        convention = "google"

        # Per-rule configuration
        [tool.ruff.per-file-ignores]
        "tests/*" = ["D103"]  # Ignore missing docstrings in tests
        "setup.py" = ["D100"]  # Ignore missing module docstring in setup.py
        ]]>
      </configuration_example>

      <best_practices>
        <standard>Run Ruff with --fix to auto-fix supported rules</standard>
        <standard>Use per-file-ignores for specific exceptions rather than global ignores</standard>
        <standard>Keep line length consistent with Black formatter (default 88 or configured 120)</standard>
        <standard>Use pre-commit hooks to enforce Ruff checks</standard>
        <standard>Document rationale for ignored rules in comments</standard>
        <standard>Run Ruff in strict mode for CI/CD pipelines</standard>
        <standard>Update Ruff configuration when adding new dependencies</standard>
      </best_practices>

      <pre_commit_example>
        <![CDATA[
        # .pre-commit-config.yaml
        repos:
        - repo: https://github.com/astral-sh/ruff-pre-commit
          rev: v0.3.4
          hooks:
            - id: ruff
              args: [--fix, --exit-non-zero-on-fix]
            - id: ruff-format
        ]]>
      </pre_commit_example>
    </ruff_rules>

    <tool_configurations>
      <standard>Document configuration options for:
        - pylint (reference: pylint.pycqa.org)
        - pyright (reference: microsoft.github.io/pyright)
        - mypy (reference: mypy.readthedocs.io)
        - commitizen (reference: commitizen-tools.github.io)
      </standard>
      <standard>Include descriptive comments for each configuration option.</standard>
    </tool_configurations>

    <test_imports>
      <standard>Import necessary pytest types in TYPE_CHECKING block:
        - CaptureFixture
        - FixtureRequest
        - LogCaptureFixture
        - MonkeyPatch
        - MockerFixture
        - VCRRequest (when using pytest-recording)
      </standard>
    </test_imports>

    <settings_management>
      <standard>Use Pydantic's BaseSettings for all configuration classes.</standard>
      <standard>Include comprehensive docstrings with examples for all settings classes.</standard>
      <standard>Group settings logically with clear section comments.</standard>
      <standard>Add descriptions to all Field definitions.</standard>
      <standard>Use SecretStr for sensitive data like passwords and tokens.</standard>
      <standard>Include proper type hints for all settings.</standard>
      <standard>Add field validators for critical settings.</standard>
      <standard>Use model validators for interdependent settings.</standard>
      <standard>Provide property methods for derived settings.</standard>
      <standard>Include configuration examples in docstrings.</standard>
      <standard>Add proper error messages for validation failures.</standard>
      <standard>Use SettingsConfigDict for environment configuration.</standard>
      <standard>Handle sensitive data properly in string representations.</standard>
      <standard>Add pyright directives on their own lines, not inline with code.</standard>
      <standard>Use proper type casting for SecretStr values.</standard>
      <example>
        <![CDATA[
        class AppSettings(BaseSettings):
            """Application settings with environment variable support.

            This class manages all configuration settings for the application,
            supporting environment variable overrides and validation.

            Key Features:
                - Environment variable configuration with prefix APP_CONFIG_
                - Automatic validation of settings
                - Secure handling of sensitive data
                - Comprehensive error messages

            Example:
                ```python
                settings = AppSettings()

                # Access settings
                api_key = settings.api_key

                # Override from environment:
                # export APP_CONFIG_API_KEY=your-key
                ```
            """
            model_config = SettingsConfigDict(
                env_prefix="APP_CONFIG_",
                env_file=(".env", ".envrc"),
                env_file_encoding="utf-8",
                extra="allow",
                arbitrary_types_allowed=True,
            )

            # API settings
            api_key: SecretStr = Field(
                default=SecretStr(""),
                description="API key for external service"
            )
            api_url: str = Field(
                default="https://api.example.com",
                description="Base URL for API"
            )

            @field_validator("api_url")
            def validate_api_url(cls, v: str) -> str:
                """Validate API URL format.

                Args:
                    v: URL to validate

                Returns:
                    str: Validated URL

                Raises:
                    ValueError: If URL is invalid
                """
                if not v.startswith(("http://", "https://")):
                    raise ValueError("API URL must start with http:// or https://")
                return v

            @property
            def auth_header(self) -> dict[str, str]:
                """Get authentication header.

                Returns:
                    dict[str, str]: Header with authentication
                """
                # pyright: reportAttributeAccessIssue=false
                return {"Authorization": f"Bearer {self.api_key.get_secret_value()}"}
        ]]>
      </example>
    </settings_management>

    <pydantic_best_practices>
      <standard>Always use the latest Pydantic v2 features and patterns.</standard>
      <standard>Use Field with descriptions for all fields.</standard>
      <standard>Implement proper validators using @field_validator and @model_validator.</standard>
      <standard>Use SecretStr for sensitive data with proper type handling.</standard>
      <standard>Add proper error messages in validators.</standard>
      <standard>Use computed fields (@computed_field) for derived values when appropriate.</standard>
      <standard>Implement proper JSON serialization methods if needed.</standard>
      <standard>Use proper type annotations with Optional and Union when needed.</standard>
      <standard>Add examples in docstrings showing usage patterns.</standard>
      <standard>Use SettingsConfigDict for configuration classes.</standard>
      <standard>Handle sensitive data properly in string representations.</standard>
      <standard>Use model_config for class-level configuration.</standard>
      <standard>Implement proper validation error handling.</standard>
      <standard>Use frozen models when immutability is needed.</standard>
      <example>
        <![CDATA[
        from pydantic import BaseModel, Field, SecretStr, field_validator, computed_field
        from typing import Optional

        class UserCredentials(BaseModel):
            """User credentials with secure password handling.

            Example:
                ```python
                creds = UserCredentials(
                    username="user",
                    password="secret"
                )
                ```
            """
            username: str = Field(
                min_length=3,
                description="Username for authentication"
            )
            password: SecretStr = Field(
                description="User password"
            )
            role: Optional[str] = Field(
                default=None,
                description="Optional user role"
            )

            @field_validator("username")
            def validate_username(cls, v: str) -> str:
                """Validate username format.

                Args:
                    v: Username to validate

                Returns:
                    str: Validated username

                Raises:
                    ValueError: If username is invalid
                """
                if not v.isalnum():
                    raise ValueError("Username must be alphanumeric")
                return v

            @computed_field
            @property
            def is_admin(self) -> bool:
                """Check if user is admin.

                Returns:
                    bool: True if user has admin role
                """
                return self.role == "admin"

            def __str__(self) -> str:
                """Get string representation with hidden password."""
                return f"UserCredentials(username={self.username}, password=****)"
        ]]>
      </example>
    </pydantic_best_practices>

    <error_handling_standards>
      <standard>Use proper error types for different validation scenarios.</standard>
      <standard>Include detailed error messages with context.</standard>
      <standard>Handle sensitive data properly in error messages.</standard>
      <standard>Use proper error boundaries in validation code.</standard>
      <standard>Add proper error recovery mechanisms.</standard>
      <standard>Include examples of error handling in docstrings.</standard>
      <example>
        <![CDATA[
        from pydantic import ValidationError

        try:
            settings = AppSettings()
        except ValidationError as e:
            logger.error(
                "Settings validation failed",
                error_details=e.errors(),
                # Exclude sensitive fields from error details
                settings={k: v for k, v in e.model_dump().items()
                         if not isinstance(v, SecretStr)}
            )
            raise ConfigurationError("Application configuration failed") from e
        ]]>
      </example>
    </error_handling_standards>

    <security_standards>
      <standard>Use SecretStr for all sensitive data.</standard>
      <standard>Never log sensitive values.</standard>
      <standard>Properly handle sensitive data in error messages.</standard>
      <standard>Use proper type casting for SecretStr values.</standard>
      <standard>Add proper access controls to sensitive settings.</standard>
      <standard>Implement proper data masking in string representations.</standard>
      <example>
        <![CDATA[
        class ApiConfig(BaseModel):
            """API configuration with secure credential handling."""
            api_key: SecretStr = Field(
                description="API key (sensitive)"
            )

            def __str__(self) -> str:
                """Get string representation with masked key."""
                return "ApiConfig(api_key=****)"

            def get_headers(self) -> dict[str, str]:
                """Get API headers with proper key handling."""
                # pyright: reportAttributeAccessIssue=false
                return {"Authorization": f"Bearer {self.api_key.get_secret_value()}"}
        ]]>
      </example>
    </security_standards>

    <pydantic_standards>
      <standard>When using pydantic, pydantic_settings, or any code that uses get_secret_value(), add the following linter directives while preserving any existing linter directives/comments (pylint, pyright, mypy, etc.) already present in the file:
          ```python
          # pylint: disable=no-name-in-module
          # pylint: disable=no-member
          # pyright: reportInvalidTypeForm=false
          # pyright: reportUndefinedVariable=false
          ```
      </standard>
      <explanation>These directives are necessary because:
          - no-name-in-module: Prevents false positives with pydantic's dynamic imports
          - no-member: Prevents false positives with dynamic attributes like get_secret_value
          - reportInvalidTypeForm: Handles pydantic's type validation patterns
          - reportUndefinedVariable: Manages pydantic's dynamic variable creation

          Note: These directives should be added alongside any existing linter directives, not replace them!!!!
      </explanation>
      <example>
        <![CDATA[
        # Existing linter directives remain untouched
        # pylint: disable=too-many-arguments
        # pyright: reportPrivateUsage=false

        # Add pydantic-specific directives
        # pylint: disable=no-name-in-module
        # pylint: disable=no-member
        # pyright: reportInvalidTypeForm=false
        # pyright: reportUndefinedVariable=false

        from pydantic import BaseModel, Field
        from pydantic_settings import BaseSettings, SettingsConfigDict

        class MySettings(BaseSettings):
            """Application settings."""
            model_config = SettingsConfigDict(
                env_prefix="APP_",
                env_file=".env",
                extra="allow"
            )
        ]]>
      </example>
    </pydantic_standards>
  </configuration_standards>

  <testing_practices>
    <fixtures>
      <standard>Use pytest fixtures for reusable test components.</standard>
      <standard>Utilize tmp_path fixture for file-based tests.</standard>
      <examples>
        <example>
          <![CDATA[
          # VCR Configuration Example
          @pytest.fixture(scope="module")
          def vcr_config() -> Dict[str, Any]:
              """Configure VCR for test recording.

              Returns:
                  VCR configuration dictionary
              """
              return {
                  "filter_headers": ["authorization", "x-api-key"],
                  "match_on": ["method", "scheme", "host", "port", "path", "query"],
                  "decode_compressed_response": True
              }

          # Discord.py Test Fixtures
          @pytest.fixture
          async def test_guild() -> AsyncGenerator[discord.Guild, None]:
              """Create a test guild.

              Yields:
                  Test guild instance
              """
              guild = await dpytest.driver.create_guild()
              await dpytest.driver.configure_guild(guild)
              yield guild
              await dpytest.empty_queue()

          @pytest.fixture
          async def test_channel(
              test_guild: discord.Guild
          ) -> AsyncGenerator[discord.TextChannel, None]:
              """Create a test channel.

              Args:
                  test_guild: Test guild fixture

              Yields:
                  Test channel instance
              """
              channel = await dpytest.driver.create_text_channel(test_guild)
              yield channel
              await dpytest.empty_queue()
          ]]>
        </example>
        <example>
          <![CDATA[
          # Async Test Examples
          @pytest.mark.asyncio
          @pytest.mark.vcr(
              filter_headers=["authorization"],
              match_on=["method", "scheme", "host", "port", "path", "query"]
          )
          async def test_agent_research(
              mocker: MockerFixture,
              test_agent: AgentExecutor,
              caplog: LogCaptureFixture
          ) -> None:
              """Test agent research functionality.

              Args:
                  mocker: Pytest mocker fixture
                  test_agent: Agent fixture
                  caplog: Log capture fixture
              """
              # Mock web search tool
              mock_search = mocker.patch(
                  "your_package.tools.web_search",
                  return_value="Test search result"
              )

              query = "What is the capital of France?"
              result, steps = await research_topic(test_agent, query)

              assert "Paris" in result.lower()
              assert len(steps) > 0
              assert mock_search.call_count > 0

          # Discord.py Command Test
          @pytest.mark.asyncio
          async def test_research_command(
              test_guild: discord.Guild,
              test_channel: discord.TextChannel,
              test_agent: AgentExecutor
          ) -> None:
              """Test Discord research command.

              Args:
                  test_guild: Test guild fixture
                  test_channel: Test channel fixture
                  test_agent: Agent fixture
              """
              await dpytest.message("?research What is Python?")

              messages = await dpytest.sent_queue.get()
              assert len(messages) == 1
              assert "programming language" in messages[0].content.lower()
          ]]>
        </example>
      </examples>
    </fixtures>

    <test_organization>
      <standard>Mirror source code directory structure in tests directory.</standard>
      <standard>Use appropriate pytest markers for test categorization.</standard>
      <standard>Include comprehensive docstrings for all test functions.</standard>
      <example>
        <![CDATA[
        @pytest.mark.slow()
        @pytest.mark.services()
        @pytest.mark.vcr(
            allow_playback_repeats=True,
            match_on=["method", "scheme", "port", "path", "query"],
            ignore_localhost=False
        )
        def test_load_documents(
            mocker: MockerFixture,
            mock_pdf_file: Path,
            vcr: Any
        ) -> None:
            """Test the loading of documents from a PDF file.

            Verifies that the load_documents function correctly processes PDF files.

            Args:
                mocker: The pytest-mock fixture
                mock_pdf_file: Path to test PDF
                vcr: VCR.py fixture
            """
            # Test implementation
        ]]>
      </example>
    </test_organization>

    <structlog_testing>
      <standard>Always use structlog's capture_logs context manager for testing log output.</standard>
      <standard>Never use pytest's caplog fixture for structlog message verification.</standard>
      <standard>Check log events using log.get("event") instead of checking message strings.</standard>
      <standard>Include descriptive error messages in log assertions.</standard>
      <standard>Remove caplog.set_level() calls when using structlog.</standard>
      <standard>For dynamic log messages containing variable content (like file paths), use startswith() or partial matching.</standard>
      <example>
        <![CDATA[
        @pytest.mark.asyncio
        async def test_example_event(bot: DemocracyBot) -> None:
            """Test example event logging.

            Args:
                bot: The Discord bot instance
            """
            with structlog.testing.capture_logs() as captured:
                # Perform the action that generates logs
                await some_action()

                # Check if the log message exists in the captured structlog events
                assert any(
                    log.get("event") == "Expected Event Message" for log in captured
                ), "Expected 'Expected Event Message' not found in logs"

                # For multiple log checks, use multiple assertions
                assert any(
                    log.get("event") == "Another Expected Event" for log in captured
                ), "Expected 'Another Expected Event' not found in logs"

                # For dynamic messages with variable content, use startswith()
                assert any(
                    log.get("event").startswith("File created at:") for log in captured
                ), "Expected file creation message not found in logs"

                # For messages containing variable paths or IDs, use partial matching
                assert any(
                    "user_123" in log.get("event") for log in captured
                ), "Expected user ID in log message not found"
        ]]>
      </example>
      <best_practices>
        <standard>Use descriptive variable names like 'captured' for the capture_logs result.</standard>
        <standard>Check exact event messages rather than using string contains when possible.</standard>
        <standard>Use startswith() for messages with known prefixes but variable content.</standard>
        <standard>Use string contains (in operator) for messages where the variable content could be anywhere.</standard>
        <standard>Include the full expected message in the assertion error message.</standard>
        <standard>Group related log checks together within the same capture_logs context.</standard>
      </best_practices>
    </structlog_testing>
  </testing_practices>

  <examples>
    <example>
      <![CDATA[
      Example folder structure:
democracy-exe/
 democracy_exe/                   # Main package directory
    __init__.py
    __main__.py
    __version__.py
    agentic/                    # Agentic system components
       __init__.py
       agents/
       workflows/
    ai/                         # AI/ML components
       __init__.py
       chains/
       models/
       tools/
    bot_logger/                 # Logging components
    chatbot/                    # Discord chatbot components
    clients/                    # API clients
    data/                       # Data storage
    exceptions/                 # Custom exceptions
    factories/                  # Factory classes
    models/                     # Data models
    shell/                      # Shell/CLI components
    subcommands/                # CLI subcommands
    utils/                      # Utility functions
    vendored/                   # Vendored dependencies
    aio_settings.py            # Async settings
    asynctyper.py              # Async CLI utilities
    base.py                    # Base classes
    cli.py                     # CLI implementation
    constants.py               # Constants
    debugger.py               # Debugging utilities
    llm_manager.py            # LLM management
    main.py                   # Main entry point
    types.py                  # Type definitions

 tests/                         # Test directory
    __init__.py
    conftest.py
    unit/
    integration/
    fixtures/

 docs/                          # Documentation
 scripts/                       # Utility scripts
 stubs/                        # Type stubs
 ai_docs/                      # AI documentation
 cookbook/                     # Code examples

 .github/                      # GitHub configuration
 .vscode/                      # VSCode configuration
 .devcontainer/               # Dev container config

 pyproject.toml               # Project configuration
 Justfile                     # Just commands
 Makefile                     # Make commands
 README.md                    # Project documentation
 CONTRIBUTING.md             # Contribution guide
 LICENSE                     # License file
 mkdocs.yml                  # Documentation config
      ]]>
    </example>
    <example>
      <![CDATA[
      Example README.md content:
      # Democracy Exe

      This repository contains a structured agentic system built with LangChain and LangGraph.

      ## Structure
      - `agents/`: Components for continuous use in agentic systems
      - `tasks/`: Components for specific task execution
      - `templates/`: Reusable component structures

      ## Usage
      [Include guidelines on how to use and contribute to the system]
      ]]>
    </example>
    <example>
      <![CDATA[
      Example prompt.xml for John Helldiver:
      <?xml version="1.0" encoding="UTF-8"?>
      <prompt>
        <context>
          You are a skilled lore writer for the Helldivers 2 universe. Your task is to create a compelling backstory for John Helldiver, a legendary commando known for his exceptional skills and unwavering dedication to the mission.
        </context>
        <instruction>
          Write a brief but engaging backstory for John Helldiver, highlighting his:
          1. Origin and early life
          2. Key missions and accomplishments
          3. Unique personality traits
          4. Signature weapons or equipment
          5. Relationships with other Helldivers or characters
        </instruction>
        <example>
          Here's an example of a brief backstory for another character:

          Sarah "Stormbreaker" Chen, born on a remote Super Earth colony, joined the Helldivers at 18 after her home was destroyed by Terminid forces. Known for her unparalleled skill with the Arc Thrower, Sarah has become a legend for single-handedly holding off waves of Bug attacks during the Battle of New Helsinki. Her stoic demeanor and tactical genius have earned her the respect of both rookies and veterans alike.
        </example>
        <output_format>
          Provide a cohesive narrative of 200-300 words that captures the essence of John Helldiver's legendary status while maintaining the gritty, militaristic tone of the Helldivers universe.
        </output_format>
      </prompt>
      ]]>
    </example>
    <example>
      <![CDATA[
      Example README.md for John Helldiver:
      # John Helldiver Backstory Prompt

      ## Purpose
      This prompt is designed to generate a compelling backstory for John Helldiver, a legendary commando in the Helldivers 2 universe. It aims to create a rich, engaging narrative that fits seamlessly into the game's lore.

      ## Usage
      1. Use this prompt with a large language model capable of creative writing and understanding context.
      2. Provide the prompt to the model without modification.
      3. The generated output should be a 200-300 word backstory that can be used as-is or as a foundation for further development.

      ## Expected Output
      A brief but detailed backstory covering John Helldiver's origin, key accomplishments, personality traits, equipment, and relationships within the Helldivers universe.

      ## Special Considerations
      - Ensure the tone matches the gritty, militaristic style of Helldivers 2.
      - The backstory should emphasize John's exceptional skills and dedication to his missions.
      - Feel free to iterate on the output, using it as a starting point for more detailed character development.
      ]]>
    </example>
      <example>
      <![CDATA[
      Example metadata.json for John Helldiver:
      {
        "promptName": "JohnHelldiverBackstory",
        "version": "1.0",
        "targetModel": "gpt4o",
        "author": "YourName",
        "creationDate": "2024-12-08",
        "lastTestedDate": "2024-12-08",
        "tags": ["Helldivers2", "lore", "character-backstory", "sci-fi"],
        "description": "Generates a backstory for John Helldiver, a legendary commando in the Helldivers 2 universe",
        "performanceMetrics": {
          "averageOutputQuality": 4.5,
          "successRate": 0.95
        },
        "promptStructure": "Four-level prompt (Context, Instruction, Example, Output Format)"
      }
      ]]>
    </example>
    <example>
      <![CDATA[
      Example examples/example1.md for John Helldiver:
      # Example Output 1: John Helldiver Backstory

      John "Hellfire" Helldiver was born in the underground bunkers of Super Earth during the height of the Bug War. Raised by veteran Helldivers, John's childhood was a brutal training regimen that forged him into a living weapon. At 16, he led his first mission against a Terminid hive, earning his call sign "Hellfire" after single-handedly destroying the hive with nothing but a flamethrower and sheer determination.

      Known for his uncanny ability to turn the tide of impossible battles, John has become a symbol of hope for humanity. His most famous exploit came during the Siege of New Atlantis, where he held off waves of Automaton forces for 72 hours straight, allowing thousands of civilians to evacuate. John's preferred loadout includes a customized Liberator assault rifle and the experimental P-7 "Punisher" sidearm, both gifts from Super Earth's top weapons engineers.

      Despite his legendary status, John remains a man of few words, letting his actions speak louder than any speech could. His unwavering loyalty to Super Earth and his fellow Helldivers is matched only by his hatred for the enemies of democracy. Rookies whisper that John Helldiver doesn't sleep; he just waits for the next drop.

      (Word count: 182)
      ]]>
    </example>
    <example>
      <![CDATA[
      Example prompt_schema.xsd:
      <?xml version="1.0" encoding="UTF-8"?>
      <xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema">
        <xs:element name="prompt">
          <xs:complexType>
            <xs:sequence>
              <xs:element name="context" type="xs:string"/>
              <xs:element name="instruction" type="xs:string"/>
              <xs:element name="example" type="xs:string"/>
              <xs:element name="output_format" type="xs:string"/>
            </xs:sequence>
          </xs:complexType>
        </xs:element>
      </xs:schema>
      ]]>
    </example>
    <example>
      <![CDATA[
      Example Justfile:
      lint:
        xmllint --schema prompt_schema.xsd prompt.xml --noout
      ]]>
    </example>
  </examples>

  <reasoning>
    <point>Hierarchical structure allows for easy navigation and scalability.</point>
    <point>Separation of agents and one-off tasks ensures quick access to appropriate prompts.</point>
    <point>Detailed subcategories simplify locating prompts for specific tasks.</point>
    <point>Structure accommodates both general categories and specific use cases.</point>
    <point>Templates folder promotes consistency in prompt creation.</point>
    <point>README file provides clear documentation for all users.</point>
  </reasoning>

  <prompt_engineering_standards>
    <output_format>
      <standard>Specify that responses should only include changed or new code snippets, not entire functions or files.</standard>
      <standard>Use diff-like format to clearly indicate additions and deletions when appropriate.</standard>
    </output_format>
    <xml_structure>
      <standard>Use clear, descriptive tag names that are self-explanatory (e.g., &lt;context&gt;, &lt;task&gt;, &lt;examples&gt;).</standard>
      <standard>Organize content hierarchically with proper nesting of tags.</standard>
      <standard>Maintain consistent tag usage throughout prompts.</standard>
      <standard>Use line breaks and indentation for readability.</standard>
    </xml_structure>

    <component_organization>
      <standard>Separate different components with distinct tags.</standard>
      <standard>Use &lt;context&gt; for background information.</standard>
      <standard>Use &lt;instructions&gt; for specific directives.</standard>
      <standard>Use &lt;examples&gt; for sample inputs and outputs.</standard>
      <standard>Use &lt;output_format&gt; to define response structure.</standard>
      <standard>Use &lt;reflection&gt; for AI thinking steps.</standard>
    </component_organization>

    <best_practices>
      <standard>Include only necessary information in each tag.</standard>
      <standard>Number or bullet point instructions for clarity.</standard>
      <standard>Use variables with descriptive names (e.g., &lt;variable_name&gt;{{value}}&lt;/variable_name&gt;).</standard>
      <standard>Combine XML tags with other prompt engineering techniques when appropriate.</standard>
      <standard>Include validation using XSD schemas for prompt structure.</standard>
    </best_practices>

    <reflection_patterns>
      <standard>Always use chain-of-thought prompting by default to improve accuracy and coherence.</standard>
      <standard>Include explicit thinking steps in prompts using &lt;thinking&gt; and &lt;answer&gt; tags.</standard>
      <standard>Break down complex tasks into clear steps.</standard>
      <standard>Use structured thinking for tasks involving:</standard>
      <list>
        <item>Complex math or logic</item>
        <item>Multi-step analysis</item>
        <item>Writing complex documents</item>
        <item>Decisions with multiple factors</item>
        <item>Research and investigation</item>
      </list>
      <example>
        <![CDATA[
        <prompt>
          <context>Analyzing a complex codebase for refactoring.</context>
          <thinking>
            1. First, I'll identify the main components and their relationships
            2. Then, I'll analyze each component for SOLID principles
            3. Finally, I'll propose specific refactoring steps
          </thinking>
          <answer>
            Provide a structured analysis with:
            - Component relationships
            - SOLID violations
            - Refactoring proposals
          </answer>
        </prompt>
        ]]>
      </example>
    </reflection_patterns>

    <variable_handling>
      <standard>Use descriptive variable names in XML tags.</standard>
      <standard>Include type hints and validation rules for variables.</standard>
      <standard>Document expected formats and constraints.</standard>
      <example>
        <![CDATA[
        <prompt>
          <variables>
            <code_snippet type="python" max_length="500">{{code_to_review}}</code_snippet>
            <style_guide type="url">{{style_guide_link}}</style_guide>
            <severity_level type="enum" values="high,medium,low">{{severity}}</severity_level>
          </variables>
          <task>Review the code according to the style guide at the specified severity level.</task>
        </prompt>
        ]]>
      </example>
    </variable_handling>
  </prompt_engineering_standards>

  <marimo_standards>
    <imports>
      <standard>All external imports must be in the first cell of marimo_* files.</standard>
      <standard>First cell should import and return all modules needed by subsequent cells.</standard>
      <standard>Use importlib.reload() for development modules that may change.</standard>
    </imports>

    <cell_definition>
      <standard>All cells must be decorated with @app.cell.</standard>
      <standard>Always use explicit tuple returns, even for single values.</standard>
      <standard>No function definitions allowed in marimo notebook files (prefix: marimo_*).</standard>
      <standard>All functions must be imported from prompt_library_module.py.</standard>
      <standard>No error handling in notebook cells - handle errors in imported functions.</standard>
      <standard>Cell parameters should only include variables actually used in the cell.</standard>
      <standard>Skip type annotations and docstrings for marimo notebook cells.</standard>
    </cell_definition>

    <state_management>
      <standard>All cell dependencies must be explicitly declared as parameters.</standard>
      <standard>Avoid mutating shared state between cells.</standard>
      <standard>Use proper typing for all state variables.</standard>
    </state_management>

    <ui_components>
      <standard>UI components should be created and modified through the reactive system.</standard>
      <standard>Use proper typing for all UI components.</standard>
      <standard>Include descriptive labels and help text.</standard>
    </ui_components>

    <error_handling>
      <standard>Use proper error boundaries and guards in each cell.</standard>
      <standard>Provide descriptive error messages with context.</standard>
      <standard>Use mo.stop() for validation guards.</standard>
    </error_handling>

    <cell_dependencies>
      <standard>All cell dependencies must be explicitly declared.</standard>
      <standard>Avoid circular dependencies between cells.</standard>
      <standard>Use proper ordering of cells based on dependencies.</standard>
    </cell_dependencies>

    <ui_styling>
      <standard>Use consistent styling objects for UI components.</standard>
      <standard>Follow Material Design principles for component styling.</standard>
      <standard>Maintain responsive design patterns.</standard>
    </ui_styling>

    <python_differences>
      <standard>Understand key differences from regular Python code.</standard>
      <standard>Follow Marimo-specific patterns for state and reactivity.</standard>
    </python_differences>

    <reactive_patterns>
      <standard>Use reactive programming patterns for UI and state updates.</standard>
      <standard>Maintain unidirectional data flow.</standard>
      <standard>Handle side effects properly in reactive contexts.</standard>
    </reactive_patterns>
  </marimo_standards>

  <cli_standards>
    <standard>Use Typer for the main APP instance with proper async support through AsyncTyperImproved when needed.</standard>
    <standard>Initialize the main APP with: APP = Typer() or APP = AsyncTyperImproved() based on async requirements.</standard>
    <standard>Load subcommands dynamically using the load_commands() function with both sync and async support.</standard>
    <standard>Place all subcommands in the subcommands directory with _cmd.py suffix (e.g., ai_docs_cmd.py).</standard>
    <standard>Each subcommand module should define its own APP instance and follow consistent naming.</standard>
    <standard>Use proper type annotations for all command parameters and return values.</standard>
    <standard>Include descriptive docstrings for all commands following Google style.</standard>
    <standard>Use Annotated for command parameters to provide help text and options.</standard>
    <standard>Implement proper signal handling for graceful shutdown (SIGTERM).</standard>
    <standard>Use rich for console output formatting and printing.</standard>
    <standard>Provide version information through dedicated commands.</standard>
    <standard>Configure entry points in pyproject.toml:
      contextforge-cli = "contextforge_cli.cli:main"
      cfctl = "contextforge_cli.cli:entry"
      contextforgectl = "contextforge_cli.cli:entry"
    </standard>

    <example>
      <![CDATA[
      # Example subcommand implementation
      from typing import Annotated
      import typer
      from rich import print as rprint

      APP = typer.Typer()

      @APP.command()
      def my_command(
          param: Annotated[str, typer.Option("--param", "-p", help="Parameter description")] = "default",
          verbose: Annotated[bool, typer.Option("--verbose", "-v", help="Show detailed output")] = False,
      ) -> None:
          """Command description following Google style.

          Args:
              param: Parameter description
              verbose: Whether to show detailed output
          """
          if verbose:
              rprint(f"[green]Running command with param: {param}[/green]")
          # Command implementation
      ]]>
    </example>

    <subcommand_organization>
      <standard>Organize subcommands by functionality (e.g., ai_docs_cmd.py for AI documentation features)</standard>
      <standard>Keep subcommand modules focused and single-purpose</standard>
      <standard>Use consistent naming pattern: {feature}_cmd.py</standard>
      <standard>Include __init__.py in subcommands directory</standard>
      <structure>
        <![CDATA[
        src/contextforge_cli/
         cli.py                 # Main CLI entry point
         subcommands/
             __init__.py
             ai_docs_cmd.py     # AI documentation commands
             {feature}_cmd.py   # Other feature commands
        ]]>
      </structure>
    </subcommand_organization>

    <command_loading>
      <standard>Use dynamic command loading through load_commands()</standard>
      <standard>Support both sync and async command loading</standard>
      <standard>Log command loading process using structlog</standard>
      <example>
        <![CDATA[
        # Dynamic command loading
        def load_commands(directory: str = "subcommands") -> None:
            """Load subcommands from the specified directory.

            Args:
                directory: The directory to load subcommands from
            """
            for filename in os.listdir(subcommands_dir):
                if filename.endswith("_cmd.py"):
                    module = import_module(module_name)
                    if hasattr(module, "APP"):
                        APP.add_typer(module.APP, name=filename[:-7])
        ]]>
      </example>
    </command_loading>
  </cli_standards>

  <discord_testing_standards>
    <test_configuration>
      <standard>Add required linter disables for Discord.py files</standard>
      <standard>Configure proper intents for test environment</standard>
      <standard>Set up test guilds with appropriate permissions</standard>
      <standard>Configure logging for test environment</standard>
      <standard>Use consistent test data across test suite</standard>

      <file_setup>
        <standard>Add necessary linter disables at the top of test files</standard>
        <example>
          <![CDATA[
          # pylint: disable=no-member
          # pylint: disable=possibly-used-before-assignment
          # pyright: reportImportCycles=false
          # mypy: disable-error-code="index"
          # mypy: disable-error-code="no-redef"
          # pyright: reportAttributeAccessIssue=false

          import pytest
          import discord
          import discord.ext.test as dpytest
          from discord.ext import commands
          from typing import AsyncGenerator, Generator
          ]]>
        </example>
      </file_setup>

      <bot_configuration>
        <standard>Set up bot with all required intents for testing</standard>
        <standard>Configure proper command prefix and settings</standard>
        <standard>Initialize bot with test-specific settings</standard>
        <example>
          <![CDATA[
          @pytest.fixture
          async def bot() -> AsyncGenerator[commands.Bot, None]:
              """Create a DemocracyBot instance for testing.

              Returns:
                  AsyncGenerator[commands.Bot, None]: DemocracyBot instance with test configuration
              """
              # Configure intents
              intents = discord.Intents.default()
              intents.members = True
              intents.message_content = True
              intents.messages = True
              intents.guilds = True

              # Create DemocracyBot with test configuration
              from democracy_exe.chatbot.bot import DemocracyBot
              bot = DemocracyBot(
                  command_prefix="?",
                  intents=intents,
                  description="Test DemocracyBot instance"
              )

              # Add test-specific error handling
              @bot.event
              async def on_command_error(ctx: commands.Context, error: Exception) -> None:
                  """Handle command errors in test environment."""
                  raise error  # Re-raise for pytest to catch

              # Setup and cleanup
              await bot._async_setup_hook()  # Required for proper initialization
              dpytest.configure(bot)
              yield bot
              await dpytest.empty_queue()

          @pytest.fixture
          async def test_guild(bot: DemocracyBot) -> AsyncGenerator[discord.Guild, None]:
              """Create a test guild.

              Args:
                  bot: DemocracyBot instance

              Yields:
                  Test guild instance
              """
              guild = await dpytest.driver.create_guild()
              await dpytest.driver.configure_guild(guild)
              yield guild
              await dpytest.empty_queue()

          @pytest.fixture
          async def test_channel(test_guild: discord.Guild) -> AsyncGenerator[discord.TextChannel, None]:
              """Create a test channel.

              Args:
                  test_guild: Test guild fixture

              Yields:
                  Test channel instance
              """
              channel = await dpytest.driver.create_text_channel(test_guild)
              yield channel
              await dpytest.empty_queue()
          ]]>
        </example>
      </bot_configuration>

      <test_data_management>
        <standard>Use consistent test data across test suite</standard>
        <standard>Create fixtures for common test data</standard>
        <standard>Clean up test data after each test</standard>
        <example>
          <![CDATA[
          @pytest.fixture
          def test_data() -> dict:
              """Provide consistent test data for bot tests.

              Returns:
                  dict: Test data dictionary
              """
              return {
                  "guild_name": "Test Guild",
                  "channel_name": "test-channel",
                  "user_name": "TestUser",
                  "role_name": "TestRole",
                  "command_prefix": "?",
                  "test_message": "Hello, bot!",
                  "test_embed": discord.Embed(
                      title="Test Embed",
                      description="Test description"
                  )
              }

          @pytest.fixture(autouse=True)
          async def cleanup_test_data() -> AsyncGenerator[None, None]:
              """Clean up test data after each test."""
              yield
              await dpytest.empty_queue()
              # Reset any modified bot state
              bot = dpytest.get_config().client
              bot.clear()
          ]]>
        </example>
      </test_data_management>

      <logging_setup>
        <standard>Configure logging for test environment using structlog</standard>
        <standard>Use structlog's capture_logs context manager for testing log output</standard>
        <standard>Never use pytest's caplog fixture for structlog message verification</standard>
        <example>
          <![CDATA[
          @pytest.fixture(autouse=True)
          def setup_logging() -> None:
              """Configure structlog for test environment."""
              import structlog

              structlog.configure(
                  processors=[
                      structlog.contextvars.merge_contextvars,
                      structlog.processors.add_log_level,
                      structlog.processors.TimeStamper(fmt="iso"),
                      structlog.processors.StackInfoRenderer(),
                      structlog.testing.capture_logs,
                  ],
                  wrapper_class=structlog.make_filtering_bound_logger("DEBUG"),
                  context_class=dict,
                  logger_factory=structlog.testing.LogCapture,
                  cache_logger_on_first_use=True
              )

          @pytest.mark.asyncio
          async def test_example_event(bot: DemocracyBot) -> None:
              """Test example event logging.

              Args:
                  bot: The Discord bot instance
              """
              with structlog.testing.capture_logs() as captured:
                  # Perform the action that generates logs
                  await some_action()

                  # Check if the log message exists in the captured structlog events
                  assert any(
                      log.get("event") == "Expected Event Message" for log in captured
                  ), "Expected 'Expected Event Message' not found in logs"

                  # For multiple log checks, use multiple assertions
                  assert any(
                      log.get("event") == "Another Expected Event" for log in captured
                  ), "Expected 'Another Expected Event' not found in logs"

                  # For dynamic messages with variable content, use startswith()
                  assert any(
                      log.get("event").startswith("File created at:") for log in captured
                  ), "Expected file creation message not found in logs"
